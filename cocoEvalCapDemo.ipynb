{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from pycocotools.coco import COCO\n",
    "from pycocoevalcap.eval import COCOEvalCap\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.io as io\n",
    "import pylab\n",
    "import os.path as osp\n",
    "pylab.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "\n",
    "import json\n",
    "from json import encoder\n",
    "encoder.FLOAT_REPR = lambda o: format(o, '.3f')\n",
    "\n",
    "# set up file names and pathes\n",
    "dataDir='.'\n",
    "result_file='result_cycle.json' #'result_baseline.json'\n",
    "with open(osp.join(dataDir, result_file)) as f:\n",
    "    results = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing results... Done.\n",
      "tokenization...\n",
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'reflen': 491308, 'guess': [434972, 356925, 278878, 200831], 'testlen': 434972, 'correct': [282561, 146832, 78904, 38804]}\n",
      "ratio: 0.885334657689\n",
      "Bleu_1: 0.571\n",
      "Bleu_2: 0.454\n",
      "Bleu_3: 0.371\n",
      "Bleu_4: 0.305\n",
      "computing METEOR score...\n",
      "METEOR: 0.266\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.607\n",
      "computing CIDEr score...\n",
      "CIDEr: 2.164\n"
     ]
    }
   ],
   "source": [
    "# create cocoEval object by taking coco and cocoRes\n",
    "cocoEval = COCOEvalCap(results)\n",
    "\n",
    "# evaluate results\n",
    "cocoEval.evaluate()\n",
    "# print output evaluation scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing results... Done.\n",
      "tokenization...\n",
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'reflen': 491308, 'guess': [430631, 352584, 274537, 196490], 'testlen': 430631, 'correct': [278383, 143109, 76319, 37263]}\n",
      "ratio: 0.876499059653\n",
      "Bleu_1: 0.561\n",
      "Bleu_2: 0.445\n",
      "Bleu_3: 0.363\n",
      "Bleu_4: 0.298\n",
      "computing METEOR score...\n",
      "METEOR: 0.261\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.600\n",
      "computing CIDEr score...\n",
      "CIDEr: 2.067\n"
     ]
    }
   ],
   "source": [
    "# create cocoEval object by taking coco and cocoRes\n",
    "cocoEval = COCOEvalCap(results)\n",
    "\n",
    "# evaluate results\n",
    "cocoEval.evaluate()\n",
    "# print output evaluation scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# demo how to use evalImgs to retrieve low score result\n",
    "evals = [eva for eva in cocoEval.evalImgs if eva['CIDEr']<30]\n",
    "print 'ground truth captions'\n",
    "imgId = evals[0]['image_id']\n",
    "annIds = coco.getAnnIds(imgIds=imgId)\n",
    "anns = coco.loadAnns(annIds)\n",
    "coco.showAnns(anns)\n",
    "\n",
    "print '\\n'\n",
    "print 'generated caption (CIDEr score %0.1f)'%(evals[0]['CIDEr'])\n",
    "annIds = cocoRes.getAnnIds(imgIds=imgId)\n",
    "anns = cocoRes.loadAnns(annIds)\n",
    "coco.showAnns(anns)\n",
    "\n",
    "img = coco.loadImgs(imgId)[0]\n",
    "I = io.imread('%s/images/%s/%s'%(dataDir,dataType,img['file_name']))\n",
    "plt.imshow(I)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot score histogram\n",
    "ciderScores = [eva['CIDEr'] for eva in cocoEval.evalImgs]\n",
    "plt.hist(ciderScores)\n",
    "plt.title('Histogram of CIDEr Scores', fontsize=20)\n",
    "plt.xlabel('CIDEr score', fontsize=20)\n",
    "plt.ylabel('result counts', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save evaluation results to ./results folder\n",
    "json.dump(cocoEval.evalImgs, open(evalImgsFile, 'w'))\n",
    "json.dump(cocoEval.eval,     open(evalFile, 'w'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
